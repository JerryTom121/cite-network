# -*- coding: utf-8 -*-
'''
This module defines several functions used to scrape article metadata, 
primarily from Scopus but also from PubMed.  

Note that these functions need a Scopus API key defined in `api_key.py`.
A new Scopus API key can be generated by registering for free [on the Scopus API page](http://dev.elsevier.com/index.html). 
'''

from collections import OrderedDict
import requests
import json
#from math import ceil
import pandas as pd
import time
import xmltodict

from api_key import MY_API_KEY

class ParseError(Exception):
    pass

def _parse_scopus_metadata(response_raw):
    '''
    Given the `requests.Response`, parse the XML metadata.
    Metadata to gather:  DOI, Scopus ID, author IDs, source ID, year, references.
    :param response_raw: XML metadata, retrieved from Scopus using requests.get
    :return: A dict of metadata:
        'doi': The paper's DOI
        'sid': The paper's Scopus ID
        'pmid': The paper's PubMed ID
        'authors': The paper's authors, as a list of Scopus author IDs
        'source': The journal, etc., the paper was published in, as a Scopus source ID
        'year': The publication year
        'references': The paper's references, as a list of Scopus IDs
        'raw': The raw XML response from the server
    '''
    # Convert the xml response to a dict to make it easier to parse
    response = xmltodict.parse(response_raw.text)
    #print 'parsed to dict'
    # This branch catches error codes in the response
    if 'service-error' in response:
        # If the resource isn't found, we'll just get a bunch of key errors;
        #  return an empty set of metadata
        if response['service-error']['status']['statusCode'] == 'RESOURCE_NOT_FOUND':
            print('\t\tResource not found error')
            return {'doi': '', 'sid': ''}
        # If something else is going on, raise an exception
        else:
            print(response)
            raise ParseError('Service error in query response')
    
    if 'abstracts-retrieval-multidoc-response' in response:
        '''
        This seems to be the top-level field if Scopus finds multiple documents 
        with the same DOI.  Based on this result:  
            http://api.elsevier.com/content/abstract/doi/10.1007/s00204-014-1298-3?apiKey=1f271dd2cf40387ab1d7e4645d36f599
        it looks like this happens when an Online First version isn't correctly
        updated, and instead a new item is added to the database.  
        Loop through them, taking the first one with bibliography entries.  
        '''
        for record in response['abstracts-retrieval-multidoc-response']['abstracts-retrieval-response']:
            try:
                if 'reference' in record['item']['bibrecord']['tail']['bibliography']:
                    response = record
                    break
            except (TypeError, KeyError):
                pass
        # If we don't find any version with a bibliography, 
        #  just go with the first one
        else:
            response = response['abstracts-retrieval-multidoc-response']['abstracts-retrieval-response'][0]        
    else:
        response = response['abstracts-retrieval-response']
    # The locations of these metadata are given in the Scopus XML documentation
    # http://ebrp.elsevier.com/pdf/Scopus_Custom_Data_Documentation_v4.pdf
    # If a field is missing, the dict raises a KeyError or TypeError 
    #  (`'NoneType' object is not subscriptable`), and so we use an empty string 
    #  instead
    try:
        doi = response['coredata']['prism:doi']
    except KeyError:
        doi = ''
    #print doi
    try:
        # The content for `dc:identifier` looks something like 'Scopus:115628'        
        sid = response['coredata']['dc:identifier']
        sid = sid.split(':')[1]
        #print sid
    except KeyError:
        sid = ''
    #print sid
    try:
        pmid = response['coredata']['pubmed-id']
    except KeyError:
        pmid = ''
    try:
        authors = []
        authors_resp = response['authors']['author']
        # If there's only one <author>, xmltodict parses the contents to an OrderedDict
        if type(authors_resp) is OrderedDict:
            authors += [authors_resp['@auid']]
        # But if there are several <author>s, xmltodict produces a list of 
        #    OrderedDicts, one for each author
        elif type(authors_resp) is list:
            for author in authors_resp:
                #print(author['@auid'])
                authors += [author['@auid']]
        else:
            raise ValueError('Problem parsing authors for doi ' + doi)
    except(KeyError, TypeError):
        authors = []
        #raise ValueError('Problem parsing authors for doi ' + doi)
    #print authors
    try:
        if 'prism:isbn' in response['coredata']:
            source = response['coredata']['prism:isbn']
        else:
            source = response['coredata']['prism:issn']
    except KeyError:
        source = ''
    #print source
    try:
        year = response['item']['bibrecord']['head']['source']['publicationyear']['@first']
        year = int(year)
        #print(year)
    except KeyError:
        year = None
    try:
        refs_response = response['item']['bibrecord']['tail']['bibliography']['reference']
        refs = []
        for ref in refs_response:
            refs += [ref['ref-info']['refd-itemidlist']['itemid']['#text']]
    except(KeyError, TypeError):
        refs = []
    #print refs
    return {'doi': doi, 'sid': sid, 'pmid': pmid, 'authors': authors, 
                'source': source, 'year': year, 'references': refs}

            
def _get_query(query):
    '''
    Get an HTTP query, with some wrapping to handle timeouts.
    :param query: The HTTP query string
    :return: The requests.get response
    '''
    # Timeout for HTTP requests
    TIMEOUT = 60
    # Delay, in seconds, after receiving a timeout
    DELAY = 3*60
    # Maximum number of attempts to make before throwing an error
    MAX_ATTEMPTS = 5
    
    attempts = 0
    while (attempts < MAX_ATTEMPTS):
        attempts += 1
        try:
            response_raw = requests.get(query, 
                            #headers = {'X-ELS-APIKey': MY_API_KEY}, 
                            timeout = TIMEOUT)
            return(response_raw)
        except requests.exceptions.Timeout:
            print('Request timed out.  Cooldown for ' + str(DELAY) + ' seconds.')
            time.sleep(DELAY)
    else:
        raise requests.exceptions.Timeout('Maximum number of requests')


def get_meta_by_doi(doi, save_raw = False):
    '''
    Retrieve metadata for a single paper from Scopus given its DOI
    :param doi: The paper's DOI
    :param save_raw: Save the raw XML response from Scopus? 
    :return: A dict of metadata:
        'doi': The paper's DOI
        'sid': The paper's Scopus ID
        'pmid': The paper's PubMed ID
        'authors': The paper's authors, as a list of Scopus author IDs
        'source': The journal, etc., the paper was published in, as a Scopus source ID
        'references': The paper's references, as a list of Scopus IDs
        'raw': The raw XML response from the server
    '''
    #print 'getting metadata for DOI ' + doi
    # Build the http query, and send it using `_get_query`
    # If DOI is missing (Pandas NaN), then just return empty metadata
    if pd.isnull(doi):
        return {'doi': '', 'sid': ''}
    base_query = 'http://api.elsevier.com/content/abstract/doi/'
    query = base_query + doi + '?' + 'apiKey=' + MY_API_KEY
    print('\t' + query)
    response_raw = _get_query(query)
    # Then parse using `_parse_scopus_metadata`
    meta = _parse_scopus_metadata(response_raw)
    # If the call asks us to save the raw response, do so; otherwise add a blank
    if save_raw:
        meta['raw'] = response_raw.text
    else:
        meta['raw'] = ''
    return meta


def get_meta_by_scopus(sid, save_raw = False):
    '''
    Retrieve metadata for a single paper from Scopus given its Scopus ID
    :param sid: The paper's Scopus ID
    :param save_raw: Save the raw XML response from Scopus? 
    :return: A dict of metadata:
        'doi': The paper's DOI
        'sid': The paper's Scopus ID
        'pmid': The paper's PubMed ID
        'authors': The paper's authors, as a list of Scopus author IDs
        'source': The journal, etc., the paper was published in, as a Scopus source ID
        'references': The paper's references, as a list of Scopus IDs
        'raw': The raw XML response from the server
    '''
    # This works just like `get_meta_by_doi`
    base_query = 'http://api.elsevier.com/content/abstract/scopus_id/'
    query = base_query + sid + '?' + 'apiKey=' + MY_API_KEY
    print('\t' + query)
    response_raw = _get_query(query)
    #print 'received response'
    meta = _parse_scopus_metadata(response_raw)
    if save_raw:
        meta['raw'] = response_raw.text
    else:
        meta['raw'] = ''
    return meta

                
def get_meta_by_pmid(pmid, save_raw = False):
    '''
    Retrieve metadata for a single paper from Scopus given its PubMed ID
    :param save_raw: Save the raw XML response from Scopus? 
    :param pmid: The paper's PubMed ID
    :return: A dict of metadata:
        'doi': The paper's DOI
        'sid': The paper's Scopus ID
        'pmid': The paper's PubMed ID
        'authors': The paper's authors, as a list of Scopus author IDs
        'source': The journal, etc., the paper was published in, as a Scopus source ID
        'references': The paper's references, as a list of Scopus IDs
        'raw': The raw XML response from the server
    '''
    base_query = 'http://api.elsevier.com/content/abstract/pubmed_id/'
    query = base_query + pmid + '?' + 'apiKey=' + MY_API_KEY
    print('\t' + query)
    response_raw = _get_query(query)
    #print 'received response'
    meta = _parse_scopus_metadata(response_raw)
    if save_raw:
        meta['raw'] = response_raw.text
    else:
        meta['raw'] = ''
    return meta


def get_pmids_by_issn(issn, since = '2010', until = '2015'):
    '''
    Use a PubMed query to retrieve the list of every item published in the given source
    :param issn: The source's ISSN
    :param since: Lower bound (exclusive) on publication year
    :param until: Upper bound (exclusive) on publication year
    
    :return: A list of PubMed IDs
    '''
    # First deal with a degenerate case
    if issn == '':
        return([])
    # Build the PubMed query
    # Start with the base URL of the API
    base_query = 'http://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?'
    # Build the search string.  
    # Note that PubMed expects ISSNs with the form NNNN-NNNN
    # TODO: handle ISBNs
    if len(issn) != 8:
        return([])
    search_string = ('term=' + issn[:4] + '-' + issn[4:] + '&' + 
        'field=journal' + '&' + 
        'mindate=' + since + '&' +
        'maxdate=' + until)
    retmax = 100000    # Max no. of items to return; PubMed's max is 100k
    query = (base_query + 'retmax=' + str(retmax) + '&' + 
                # Get the results in json
                'retmode=json' + '&' +
                search_string)
    print('\t' + query)
    try:
        response_raw = _get_query(query)
        response = json.loads(response_raw.text)
        # Get the total number of items
        total_papers = int(response['esearchresult']['count'])
    except:
        # If there's an error, output the query for debugging and pass on the error
        print(query)
        raise
    # Update stdout
    print('found ' + str(total_papers) + ' items for source ' + issn)
    # This function assumes we can get everything on one page. 
    # Raise an error if this isn't the case. 
    if total_papers > retmax:
        raise ValueError('more items than PubMed can return on one page')
    pmids = response['esearchresult']['idlist']
    return(pmids)

